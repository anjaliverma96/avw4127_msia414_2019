# -*- coding: utf-8 -*-
"""LSTM_movie_subset_final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e22WicxAs4gtvlzYoeWa8kiiQk6jrBiW
"""

# import necessary libraries
import pandas as pd
import numpy as np
from numpy import array
from numpy import asarray
from numpy import zeros

import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
en_stop = set(nltk.corpus.stopwords.words('english'))

from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten, LSTM
from keras.layers import GlobalMaxPooling1D
from keras.models import Model
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.layers import Input
from keras.layers.merge import Concatenate

import re
import pickle
import matplotlib.pyplot as plt

from keras import backend as K
from keras.models import load_model

# read the data in from the csv
movies = pd.read_csv("movies_small_subset_df.csv")
movies = movies[['MovieID', 'MovieName', 'Genre', 'Plot',
       'clean_plot_text']]

# Function for converting genre column to a list such that it can be indexed to get genres
def format_list(x):
    x = x.replace("'","")
    x = x.replace("[","")
    x = x.replace("]","")
    x = x.split(',')
    result = []
    for word in x:
        result.append(word.strip())
    return result
movies["Genre"] = movies["Genre"].apply(lambda x : format_list(x))
movies.head(5)

movies.shape

# get a list of all unique genres from genres column in dataframe
unique_genre_list = list(set([a for b in movies.Genre.tolist() for a in b]))

for i in range(len(unique_genre_list)):
  movies[unique_genre_list[i]] = pd.Series([0 for x in range(len(movies.index))], index=movies.index)

movies.head(2)

movies.shape

for gen in unique_genre_list:
  movies[gen] = movies["Genre"].apply(lambda x : (pd.Series([gen]).isin(x)).astype(int))
movies.head(5)

#movies.to_csv("movies_one_hot_df.csv")
#movies = pd.read_csv("movies_one_hot_df.csv")

sentences = list(movies["clean_plot_text"])
sentence_list = [ sen.split(' ') for sen in sentences]
plt.hist([len(s) for s in sentence_list], bins=50)
plt.xlabel('Count of words')
plt.ylabel('Frequency of Sentences')

plt.show()

X = []
for sen in sentences:
    X.append(sen)
# Create output set (target/labels)
y = movies[unique_genre_list].values

# Split it into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=100)

X_train1 = list(str(elem) for elem in X_train)
X_test1 = list(str(elem) for elem in X_test)

tokenizer_movie = Tokenizer(num_words=5000)
tokenizer_movie.fit_on_texts(X_train1)

# saving
with open('tokenizer_movie_final.pickle', 'wb') as handle:
    pickle.dump(tokenizer_movie, handle, protocol=pickle.HIGHEST_PROTOCOL)
# loading
with open('tokenizer_movie_final.pickle', 'rb') as handle:
    tokenizer_movie = pickle.load(handle)

X_train1 = tokenizer_movie.texts_to_sequences(X_train1)
X_test1 = tokenizer_movie.texts_to_sequences(X_test1)

vocab_size = len(tokenizer_movie.word_index) + 1

maxlen = 500

X_train1 = pad_sequences(X_train1, padding='post', maxlen=maxlen)
X_test1 = pad_sequences(X_test1, padding='post', maxlen=maxlen)

# Define helper functions to get pre-trained glove word vector embeddings 
# and create an embeddings matrix

def get_word_embeddings():
    embeddings_dictionary = dict()
    glove_file = open('glove.6B.100d.txt', encoding="utf8")
    
    for line in glove_file:
        records = line.split()
        word = records[0]
        vector_dimensions = asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions
    
    glove_file.close()
    return embeddings_dictionary
    
embeddings_dictionary = get_word_embeddings()

def get_embedding_matrix():
    embedding_matrix = zeros((vocab_size, 100))
    for word, index in tokenizer_movie.word_index.items():
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
    return embedding_matrix

embedding_matrix = get_embedding_matrix()

# Define functions to be able to calculate additional metrics like precision,recall, f-score

def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

def precision_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def hamming_loss(y_true, y_pred):
  return K.mean(y_true*(1-y_pred)+(1-y_true)*y_pred)

# Approach 1
# Use a single dense layer with six outputs with sigmoid activation functions and binary cross entropy loss functions. 
# Each neuron in the output dense layer will represent one of the six output labels. 
# ACTIVATION : SIGMOID
from tensorflow import set_random_seed
set_random_seed(1)
deep_inputs_single = Input(shape=(maxlen,))
embedding_layer_single = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs_single)
LSTM_Layer_1_single = LSTM(128)(embedding_layer_single)
dense_layer_1_single = Dense(8, activation='sigmoid')(LSTM_Layer_1_single)
model_movie_single = Model(inputs=deep_inputs_single, outputs=dense_layer_1_single)

model_movie_single.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m, hamming_loss])

print(model_movie_single.summary())

history_movie_single_5 = model_movie_single.fit(X_train1, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.2)

loss, accuracy, f1_score, precision, recall, hamming = model_movie_single.evaluate(X_test1, y_test, verbose=1)

print("Test Score:", loss)
print("Test Accuracy:", accuracy)
print("Test Precision:", precision)
print("Test Recall:", recall)
print("Test F1-score:", f1_score)
print("Test hamming_loss:", hamming)

model_movie_single.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m, hamming_loss])
history_movie_single_10 = model_movie_single.fit(X_train1, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)

loss, accuracy, f1_score, precision, recall, hamming = model_movie_single.evaluate(X_test1, y_test, verbose=1)

print("Test Score:", loss)
print("Test Accuracy:", accuracy)
print("Test Precision:", precision)
print("Test Recall:", recall)
print("Test F1-score:", f1_score)
print("Test hamming_loss:", hamming)

model_movie_single.save("movie_lstm_single_10.h5")

# load model from single file
movie_lstm_single_10 = load_model('movie_lstm_single_10.h5', custom_objects={'f1_m': f1_m, 'recall_m':recall_m, 'precision_m':precision_m})

# Approach 1 with softmax activation
# Use a single dense layer with six outputs with sigmoid activation functions and binary cross entropy loss functions. 
# Each neuron in the output dense layer will represent one of the six output labels. 
# ACTIVATION : SOFTMAX
from tensorflow import set_random_seed
set_random_seed(1)
deep_inputs_single = Input(shape=(maxlen,))
embedding_layer_single = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs_single)
LSTM_Layer_1_single = LSTM(128)(embedding_layer_single)
dense_layer_1_single = Dense(8, activation='softmax')(LSTM_Layer_1_single)
model_movie_single_soft = Model(inputs=deep_inputs_single, outputs=dense_layer_1_single)

model_movie_single_soft.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m, hamming_loss])

history_movie_single_5_soft = model_movie_single_soft.fit(X_train1, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.2)

loss, accuracy, f1_score, precision, recall, hamming = model_movie_single_soft.evaluate(X_test1, y_test, verbose=1)

print("Test Score:", loss)
print("Test Accuracy:", accuracy)
print("Test Precision:", precision)
print("Test Recall:", recall)
print("Test F1-score:", f1_score)
print("Test hamming_loss:", hamming)

