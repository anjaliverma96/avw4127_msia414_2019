# -*- coding: utf-8 -*-
"""lstm_toxic_single+multiple_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5EWGnSKnJrDzP50prX8a2J_PJxgGE9Q
"""

# import necessary libraries
import pandas as pd
import numpy as np
from numpy import array
from numpy import asarray
from numpy import zeros
import pickle

import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
en_stop = set(nltk.corpus.stopwords.words('english'))

from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten, LSTM
from keras.layers import GlobalMaxPooling1D
from keras.models import Model
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.layers import Input
from keras.layers.merge import Concatenate
from keras import backend as K
from keras.models import load_model

import re
import matplotlib.pyplot as plt

# Read in the dataset
toxic_comments = pd.read_csv("toxic_comments_cleaned_df.csv")

print(toxic_comments.shape)

toxic_comments.head()

# remove any records where any row contains a null value or empty string
filter = toxic_comments["comment_text"] != ""
toxic_comments = toxic_comments[filter]
toxic_comments = toxic_comments.dropna()

toxic_comments.shape

# Look at the distribution of labels
def plot_label_frequency(df, label_columns):
    toxic_comments_labels = df[label_columns]
    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0] = 10
    fig_size[1] = 8
    plt.rcParams["figure.figsize"] = fig_size
    bar_plot = toxic_comments_labels.sum(axis=0).plot.bar()
    return bar_plot

label_columns = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]
plot_label_frequency(toxic_comments, label_columns)

# Create input set(feature variables) -  clean all the comments and store them in a variable named X
X = []
sentences = list(toxic_comments["clean_comment_text"])
for sen in sentences:
    X.append(sen)
# X = pd.read_csv("X_toxic_feature_google_col.csv")["colummn"].tolist()
# Create output set (target/labels)
y = toxic_comments[label_columns].values

sentences = list(toxic_comments["comment_text"])
sentence_list = [ sen.split(' ') for sen in sentences]
plt.hist([len(s) for s in sentence_list], bins=50)
plt.xlabel('Count of words')
plt.ylabel('Frequency of Sentences')

plt.show()

# Split it into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=100)

X_train1 = list(str(elem) for elem in X_train)
X_test1 = list(str(elem) for elem in X_test)
# y_train1 = list(str(elem) for elem in y_train)
# y_test1 = list(str(elem) for elem in y_test)

tokenizer_toxic = Tokenizer(num_words=5000)
tokenizer_toxic.fit_on_texts(X_train1)

# saving
with open('tokenizer_toxic_final.pickle', 'wb') as handle:
    pickle.dump(tokenizer_toxic, handle, protocol=pickle.HIGHEST_PROTOCOL)

# loading
with open('tokenizer_toxic_final.pickle', 'rb') as handle:
    tokenizer_toxic = pickle.load(handle)

X_train1 = tokenizer_toxic.texts_to_sequences(X_train1)
X_test1 = tokenizer_toxic.texts_to_sequences(X_test1)

vocab_size = len(tokenizer_toxic.word_index) + 1

maxlen = 200

X_train1 = pad_sequences(X_train1, padding='post', maxlen=maxlen)
X_test1 = pad_sequences(X_test1, padding='post', maxlen=maxlen)

# Define helper functions to get pre-trained glove word vector embeddings 
# and create an embeddings matrix

def get_word_embeddings():
    embeddings_dictionary = dict()
    glove_file = open('glove.6B.50d.txt', encoding="utf8")
    
    for line in glove_file:
        records = line.split()
        word = records[0]
        vector_dimensions = asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions
    
    glove_file.close()
    return embeddings_dictionary
    
embeddings_dictionary = get_word_embeddings()

def get_embedding_matrix():
    embedding_matrix = zeros((vocab_size, 50))
    for word, index in tokenizer_toxic.word_index.items():
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
    return embedding_matrix

embedding_matrix = get_embedding_matrix()

embedding_matrix.shape

embedding_matrix.shape

# Define functions to be able to calculate additional metrics like precision,recall, f-score

def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

def precision_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))
def hamming_loss(y_true, y_pred):
  return K.mean(y_true*(1-y_pred)+(1-y_true)*y_pred)

# Approach 1
# Use a single dense layer with six outputs with sigmoid activation functions and binary cross entropy loss functions. 
# Each neuron in the output dense layer will represent one of the six output labels. 
from tensorflow import set_random_seed
set_random_seed(1)
deep_inputs_single = Input(shape=(maxlen,))
embedding_layer_single = Embedding(vocab_size, 50, weights=[embedding_matrix], trainable=False)(deep_inputs_single)
LSTM_Layer_1_single = LSTM(128)(embedding_layer_single)
dense_layer_1_single = Dense(6, activation='sigmoid')(LSTM_Layer_1_single)
model_toxic_single = Model(inputs=deep_inputs_single, outputs=dense_layer_1_single)

model_toxic_single.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m, hamming_loss])

print(model_toxic_single.summary())

from keras.utils import plot_model
plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)

history_toxic_single = model_toxic_single.fit(X_train1, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.2)

model_toxic_single.save("toxic_lstm_single_5.h5")

# load model from single file
toxic_lstm_single_5 = load_model('toxic_lstm_single_5.h5', custom_objects={'f1_m': f1_m, 'recall_m':recall_m, 'precision_m':precision_m,
                                                                           'hamming_loss': hamming_loss})

loss, accuracy, f1_score, precision, recall, hamming = model_toxic_single.evaluate(X_test1, y_test, verbose=1)

print("Test Score:", loss)
print("Test Accuracy:", accuracy)
print("Test Precision:", precision)
print("Test Recall:", recall)
print("Test F1-score:", f1_score)
print("Test hamming_loss:", hamming)

model_toxic_single.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m, hamming_loss])
history_toxic_single_10 = model_toxic_single.fit(X_train1, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)

model_toxic_single.save("toxic_lstm_single_10.h5")

# load model from single file
toxic_lstm_single_10 = load_model('toxic_lstm_single_10.h5', custom_objects={'f1_m': f1_m, 'recall_m':recall_m, 'precision_m':precision_m})

loss, accuracy, f1_score, precision, recall, hamming = model_toxic_single.evaluate(X_test1, y_test, verbose=1)

print("Test Score:", loss)
print("Test Accuracy:", accuracy)
print("Test Precision:", precision)
print("Test Recall:", recall)
print("Test F1-score:", f1_score)
print("Test hamming_loss:", hamming)

# Approach 1 with ACTIVATION SOFTMAX
# Use a single dense layer with six outputs with sigmoid activation functions and binary cross entropy loss functions. 
# Each neuron in the output dense layer will represent one of the six output labels. 
from tensorflow import set_random_seed
set_random_seed(1)
deep_inputs_single = Input(shape=(maxlen,))
embedding_layer_single = Embedding(vocab_size, 50, weights=[embedding_matrix], trainable=False)(deep_inputs_single)
LSTM_Layer_1_single = LSTM(128)(embedding_layer_single)
dense_layer_1_single = Dense(6, activation='softmax')(LSTM_Layer_1_single)
model_toxic_single_soft = Model(inputs=deep_inputs_single, outputs=dense_layer_1_single)

model_toxic_single_soft.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m, hamming_loss])

history_toxic_single_epoch = model_toxic_single_soft.fit(X_train1, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.2)

loss, accuracy, f1_score, precision, recall, hamming = model_toxic_single_soft.evaluate(X_test1, y_test, verbose=1)

print("Test Score:", loss)
print("Test Accuracy:", accuracy)
print("Test Precision:", precision)
print("Test Recall:", recall)
print("Test F1-score:", f1_score)
print("Test hamming_loss:", hamming)

# saving
with open('tokenizer_toxic.pickle', 'wb') as handle:
    pickle.dump(tokenizer_toxic, handle, protocol=pickle.HIGHEST_PROTOCOL)

# loading
with open('tokenizer_toxic.pickle', 'rb') as handle:
    tokenizer_toxic = pickle.load(handle)

# Approach 2
# Create one dense output layer for each label. 
# Total of 6 dense layers in the output. Each layer will have its own sigmoid function.

layers_dict = {}
for i in range(len(label_columns)):
  layers_dict["y"+str(i+1)+"_train"] = y_train[:,[i]]
  layers_dict["y"+str(i+1)+"_test"] = y_test[:,[i]]

layers_dict["y1_train"]

input_1 = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)
LSTM_Layer1 = LSTM(128)(embedding_layer)

output1 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output2 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output3 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output4 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output5 = Dense(1, activation='sigmoid')(LSTM_Layer1)
output6 = Dense(1, activation='sigmoid')(LSTM_Layer1)

model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m,precision_m, recall_m])

print(model.summary())

history = model.fit(x=X_train1, y=[layers_dict["y1_train"], 
                                   layers_dict["y2_train"], 
                                   layers_dict["y3_train"], 
                                   layers_dict["y4_train"], 
                                   layers_dict["y5_train"], 
                                   layers_dict["y6_train"]], batch_size=8192, epochs=5, verbose=1, validation_split=0.2)

eval = model.evaluate(x=X_test1, y=[layers_dict["y1_test"], 
                                   layers_dict["y2_test"], 
                                   layers_dict["y3_test"], 
                                   layers_dict["y4_test"], 
                                   layers_dict["y5_test"], 
                                   layers_dict["y6_test"]], verbose=1)

# print("Test Score:", loss)
# print("Test Accuracy:", accuracy)
# print("Test Precision:", precision)
# print("Test Recall:", recall)
# print("Test F1-score:", f1_score)

